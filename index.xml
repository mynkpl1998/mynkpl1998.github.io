<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Mayank Kumar Pal</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Mayank Kumar Pal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Apr 2024 22:50:09 -0700</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>About</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Wed, 10 Apr 2024 22:50:09 -0700</pubDate>
      <guid>http://localhost:1313/about/</guid>
      <description></description>
    </item>
    <item>
      <title>Bit Manipulation</title>
      <link>http://localhost:1313/blog/bit_manipulation/</link>
      <pubDate>Mon, 08 Apr 2024 21:47:41 -0700</pubDate>
      <guid>http://localhost:1313/blog/bit_manipulation/</guid>
      <description>Bitwise Operators The following are the bitwise operators which are available in C/C++ programming language.
Operator Meaning &amp;amp; Bitwise AND | Bitwise OR ^ Bitwise XOR ~ Bitwise complement &amp;laquo; Shift left &amp;raquo; Shift right XOR The main property of XOR is that it keep the bit same if both operands are same, else it flips the bit.
A B Result 0 0 0 0 1 1 1 0 1 1 1 0 Few Properties of XOR Property Result A ^ 0 A A ^ 1 ~A A ^ A 0 A ^ A ^ A A A ^ B ^ A B A ^ B ^ B A Few properties of Shift operators.</description>
    </item>
    <item>
      <title>Dissecting Policy Optimization</title>
      <link>http://localhost:1313/blog/disecting_policy_gradients/</link>
      <pubDate>Sat, 27 Jun 2020 01:47:28 -0700</pubDate>
      <guid>http://localhost:1313/blog/disecting_policy_gradients/</guid>
      <description>I assume the reader is familiar with the basics of Reinforcement learning and has a basic understanding of statistics and a bit of calculus. One should be comfortable with manipulating value functions, policy, and bellman equations.
The main idea of writing this blog post is to summarize and extend the understanding of reinforcement learning methods that directly optimizes policy. More or less, this blog post is a summary for me to revisit the concepts and various tricks that are helpful while dealing with Policy-based optimization.</description>
    </item>
    <item>
      <title>hello</title>
      <link>http://localhost:1313/publications/itsc_reinforcement_learning_jointly_adapt_vehicular_comm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/itsc_reinforcement_learning_jointly_adapt_vehicular_comm/</guid>
      <description></description>
    </item>
    <item>
      <title>Personal Projects</title>
      <link>http://localhost:1313/projects/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/</guid>
      <description>Note : Please follow the provided link(s) in each project for full details. Also, some of the project title have hyperlink which navigates directly to the project page.
Visualizing Deep Learning Optimization Algorithms Description: Gradient based algorithms is the key to optimize the deep neural networks. Apart from the vanilla gradient descent algorithm, there exists many variant of gradient based algorithms which can improve the speed of convergence and can avoid local minima too.</description>
    </item>
  </channel>
</rss>
