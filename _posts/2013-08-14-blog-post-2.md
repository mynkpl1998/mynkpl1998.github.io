---
title: 'Deep Deterministic Policy Gradients (DDPG)'
date: 2019-10-10
permalink: /posts/ddpg
tags:
  - DDPG
  - DQN
  - off-policy
  - continous-actions-space
---

Orignal Paper Link - [Continous Control With Deep Reinforcement
Learning](https://arxiv.org/pdf/1509.02971.pdf)

### Introduction

This paper extends the Deep-Q learning algorithms to make it compatible with continuous action spaces. A deterministic policy can be learned by making use of actor-critic architecture. The algorithm was tested on the suite of tasks which involved continuous action spaces and was able to find reasonably good policies from both low-dimensional and high-dimensional sensory inputs. 

In a nutshell, this paper introduces an off-policy, actor-critic algorithm which can make use multi-layered function approximator to learn control policies for continuous action spaces tasks.


### Why continous action-spaces ?

Many physical control tasks have continuous actions-spaces. Existing approaches such as DQN can't be directly applied to these tasks as the algorithms requires finding maximum action-value to create target value needed to make an update to the parameters of Q-network. One way is to discretize the continuous action-space, which results in the curse of dimensionality. Further, control problems often require precise control and discretizing them will take this preciseness. Additionally, naive discretization of action spaces needlessly throws away information about the structure of the action domain, which may be essential for solving many problems.


### Background

An agent interact with the environment $E$ at discrete time steps. At time step $t$, agent recieves observation $x_{t}$ based upon which it selects an actions $a_{t} \in \mathcal{R}^{N}$ which environment perfoms. After performing, it return a tuple containing reward $r_{t}$, next-state $s_{t+1}$ and episode terminal status $d$.

The agent is a function mapping which maps state to action denoted by $\pi : \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})$