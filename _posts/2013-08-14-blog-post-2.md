---
title: 'Deep Deterministic Policy Gradients (DDPG)'
date: 2019-10-10
permalink: /posts/ddpg
tags:
  - DDPG
  - DQN
  - off-policy
  - continous-actions-space
---

Orignal Paper Link - [Continous Control With Deep Reinforcement
Learning](https://arxiv.org/pdf/1509.02971.pdf)

### Introduction

This paper extends the Deep-Q learning algorithms to make it compatible with continuous action spaces. A deterministic policy can be learned by making use of actor-critic architecture. The algorithm was tested on the suite of tasks which involved continuous action spaces and was able to find reasonably good policies from both low-dimensional and high-dimensional sensory inputs. 

In a nutshell, this paper introduces an off-policy, actor-critic algorithm which can make use multi-layered function approximator to learn control policies for continuous action spaces tasks.


### Why continous action-spaces ?

Many physical control tasks have continuous actions-spaces. Existing approaches such as DQN can't be directly applied to these tasks as the algorithms requires finding maximum action-value to create target value needed to make an update to the parameters of Q-network. One way is to discretize the continuous action-space, which results in the curse of dimensionality. Further, control problems often require precise control and discretizing them will take this preciseness. Additionally, naive discretization of action spaces needlessly throws away information about the structure of the action domain, which may be essential for solving many problems.


### Background

An agent interact with the environment $E$ at discrete time steps. At time step $t$, agent recieves observation $x_{t}$ based upon which it selects an actions $a_{t} \in \mathcal{R}^{N}$ which environment perfoms. In turn, it returns a tuple containing scalar reward $r_{t}$, next-state $s_{t+1}$ and episode terminal status $d$. Here, we are interested in continous-action spaces and hence $a_{t}$ belongs to real space.
The agent's behaviour is defined by the policy $\pi$. This policy is function mapping which maps state to action denoted by $\pi : \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})$. The interaction between agent and environment is modelled by Markov decision process where $\mathcal{S}$ denotes state space, $\mathcal{A} = \mathcal{R}^{N}$ is the action-space, $p(s_{1})$ is intial state distribution, $p(s_{t+1}| s_{t}, a_{t})$ is transition dynamics and $r(s_{t}, a_{t})$ is the reward function.

The return from a state is defined as given below where $\gamma \in [0, 1]$ is the discount factor.  
\begin{equation}
  \label{eq:return}
  R_{t} = \sum_{i=t}^{T} \gamma^{(i-t)} r(s_{i}, a_{i})
\end{equation}

The eqn. \ref{eq:return} depends upon the current state and the action-choosen by the policy. If policy $\pi$ is stochastic, then eqn. \ref{eq:return} has uncertanity in both action and next-state. Hence, it is a random variable. Therefore, a natural goal of reinforcement learning is to find a policy which maximizes the expected return from start distribution (formally defined below via eqn).

\\[ J = \mathbb{E}_{r_{i}} \\]